import streamlit as st
import os
from datetime import datetime
import json
from pathlib import Path
import hashlib

# PDFã¨DOCXå‡¦ç†ç”¨
try:
    from PyPDF2 import PdfReader
except ImportError:
    PdfReader = None

try:
    from docx import Document
except ImportError:
    Document = None

# OpenAI API
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (ChromaDB)
try:
    import chromadb
    from chromadb.config import Settings
except ImportError:
    chromadb = None

# åŸ‹ã‚è¾¼ã¿ç”¨
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None


class CompanyKnowledgeBase:
    """ç¤¾å†…æƒ…å ±ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, persist_directory="./company_db"):
        self.persist_directory = persist_directory
        self.documents = []
        
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if chromadb:
            self.chroma_client = chromadb.Client(Settings(
                persist_directory=persist_directory,
                anonymized_telemetry=False
            ))
            try:
                self.collection = self.chroma_client.get_collection("company_docs")
            except:
                self.collection = self.chroma_client.create_collection("company_docs")
        else:
            self.chroma_client = None
            self.collection = None
            
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if SentenceTransformer:
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        else:
            self.embedding_model = None
    
    def extract_text_from_pdf(self, file_path):
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        if not PdfReader:
            return "PDFãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            return f"PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def extract_text_from_docx(self, file_path):
        """DOCXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        if not Document:
            return "DOCXãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            doc = Document(file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            return text
        except Exception as e:
            return f"DOCXèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def extract_text_from_txt(self, file_path):
        """TXTãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='shift-jis') as f:
                    return f.read()
            except Exception as e:
                return f"TXTèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
        except Exception as e:
            return f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def add_document(self, file_path, file_name):
        """æ–‡æ›¸ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ """
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        ext = Path(file_path).suffix.lower()
        
        if ext == '.pdf':
            text = self.extract_text_from_pdf(file_path)
        elif ext == '.docx':
            text = self.extract_text_from_docx(file_path)
        elif ext == '.txt':
            text = self.extract_text_from_txt(file_path)
        else:
            return False, "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™"
        
        if not text or text.startswith("ã‚¨ãƒ©ãƒ¼") or text.startswith("ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"):
            return False, text
        
        # æ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰² (ç´„500æ–‡å­—ã”ã¨)
        chunks = self._split_text(text, chunk_size=500)
        
        # ChromaDBã«ä¿å­˜
        if self.collection and self.embedding_model:
            doc_id = hashlib.md5(file_name.encode()).hexdigest()
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_{i}"
                embedding = self.embedding_model.encode(chunk).tolist()
                
                self.collection.add(
                    embeddings=[embedding],
                    documents=[chunk],
                    metadatas=[{
                        "file_name": file_name,
                        "chunk_id": i,
                        "timestamp": datetime.now().isoformat()
                    }],
                    ids=[chunk_id]
                )
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.documents.append({
            "file_name": file_name,
            "chunks": len(chunks),
            "timestamp": datetime.now().isoformat(),
            "text_preview": text[:200]
        })
        
        return True, f"æ–‡æ›¸ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"
    
    def _split_text(self, text, chunk_size=500):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        chunks = []
        sentences = text.split('ã€‚')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def search(self, query, top_k=3):
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢"""
        if not self.collection or not self.embedding_model:
            return []
        
        try:
            query_embedding = self.embedding_model.encode(query).tolist()
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            return results
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def get_document_list(self):
        """ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹æ–‡æ›¸ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.documents


class CompanyAIAssistant:
    """ç¤¾å†…æƒ…å ±ç‰¹åŒ–å‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"""
    
    def __init__(self, knowledge_base, api_key=None):
        self.knowledge_base = knowledge_base
        self.api_key = api_key
        
        if OpenAI and api_key:
            self.client = OpenAI(api_key=api_key)
        else:
            self.client = None
    
    def generate_answer(self, question, context_docs):
        """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆ"""
        if not self.client:
            return "OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = "\n\n".join([
            f"ã€å‚è€ƒè³‡æ–™{i+1}ã€‘\n{doc}"
            for i, doc in enumerate(context_docs)
        ])
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        system_prompt = """ã‚ãªãŸã¯ç¤¾å†…æƒ…å ±ã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸç¤¾å†…æ–‡æ›¸ã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯ä¸æ˜ã§ã™ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        
        user_prompt = f"""ä»¥ä¸‹ã®ç¤¾å†…æ–‡æ›¸ã‚’å‚ç…§ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ç¤¾å†…æ–‡æ›¸ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"


def main():
    st.set_page_config(
        page_title="ç¤¾å†…æƒ…å ±ç‰¹åŒ–å‹AIæ¤œç´¢",
        page_icon="ğŸ¢",
        layout="wide"
    )
    
    st.title("ğŸ¢ ç¤¾å†…æƒ…å ±ç‰¹åŒ–å‹AIæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'knowledge_base' not in st.session_state:
        st.session_state.knowledge_base = CompanyKnowledgeBase()
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç®¡ç†
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # OpenAI APIã‚­ãƒ¼å…¥åŠ›
        api_key = st.text_input(
            "OpenAI APIã‚­ãƒ¼",
            type="password",
            help="GPT-4ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        
        if api_key:
            st.session_state.api_key = api_key
        
        st.markdown("---")
        st.header("ğŸ“š æ–‡æ›¸ç®¡ç†")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_files = st.file_uploader(
            "ç¤¾å†…æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['pdf', 'docx', 'txt'],
            accept_multiple_files=True,
            help="PDF, DOCX, TXTãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™"
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if st.button(f"ğŸ“„ {uploaded_file.name} ã‚’è¿½åŠ ", key=uploaded_file.name):
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                    temp_path = f"./temp_{uploaded_file.name}"
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
                    success, message = st.session_state.knowledge_base.add_document(
                        temp_path,
                        uploaded_file.name
                    )
                    
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                    
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
        
        st.markdown("---")
        
        # ç™»éŒ²æ–‡æ›¸ä¸€è¦§
        st.subheader("ğŸ“‹ ç™»éŒ²æ¸ˆã¿æ–‡æ›¸")
        docs = st.session_state.knowledge_base.get_document_list()
        
        if docs:
            for doc in docs:
                with st.expander(f"ğŸ“„ {doc['file_name']}"):
                    st.write(f"**ãƒãƒ£ãƒ³ã‚¯æ•°:** {doc['chunks']}")
                    st.write(f"**ç™»éŒ²æ—¥æ™‚:** {doc['timestamp']}")
                    st.write(f"**ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
                    st.text(doc['text_preview'])
        else:
            st.info("ã¾ã æ–‡æ›¸ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    st.header("ğŸ’¬ AIæ¤œç´¢ãƒãƒ£ãƒƒãƒˆ")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.chat_history:
            if message['role'] == 'user':
                st.markdown(f"**ğŸ‘¤ ã‚ãªãŸ:** {message['content']}")
            else:
                st.markdown(f"**ğŸ¤– AI:** {message['content']}")
                if 'sources' in message:
                    with st.expander("ğŸ“š å‚è€ƒã«ã—ãŸæ–‡æ›¸"):
                        for source in message['sources']:
                            st.text(source)
            st.markdown("---")
    
    # è³ªå•å…¥åŠ›
    question = st.text_input(
        "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="ä¾‹: çµŒè²»ç”³è«‹ã®æ‰‹é †ã‚’æ•™ãˆã¦ãã ã•ã„",
        key="question_input"
    )
    
    col1, col2 = st.columns([1, 5])
    with col1:
        search_button = st.button("ğŸ” æ¤œç´¢", type="primary", use_container_width=True)
    with col2:
        clear_button = st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", use_container_width=True)
    
    if clear_button:
        st.session_state.chat_history = []
        st.rerun()
    
    if search_button and question:
        if not st.session_state.get('api_key'):
            st.error("âš ï¸ OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        else:
            with st.spinner("æ¤œç´¢ä¸­..."):
                # é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
                results = st.session_state.knowledge_base.search(question, top_k=3)
                
                if results and results.get('documents') and results['documents'][0]:
                    # AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’åˆæœŸåŒ–
                    assistant = CompanyAIAssistant(
                        st.session_state.knowledge_base,
                        st.session_state.api_key
                    )
                    
                    # å›ç­”ã‚’ç”Ÿæˆ
                    context_docs = results['documents'][0]
                    answer = assistant.generate_answer(question, context_docs)
                    
                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                    st.session_state.chat_history.append({
                        'role': 'user',
                        'content': question
                    })
                    st.session_state.chat_history.append({
                        'role': 'assistant',
                        'content': answer,
                        'sources': context_docs
                    })
                    
                    st.rerun()
                else:
                    st.warning("é–¢é€£ã™ã‚‹ç¤¾å†…æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ–‡æ›¸ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("""
    ### ğŸ“– ä½¿ã„æ–¹
    1. **ã‚µã‚¤ãƒ‰ãƒãƒ¼**ã‹ã‚‰OpenAI APIã‚­ãƒ¼ã‚’è¨­å®š
    2. ç¤¾å†…æ–‡æ›¸ï¼ˆPDF/DOCX/TXTï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è¿½åŠ 
    3. ãƒãƒ£ãƒƒãƒˆæ¬„ã§è³ªå•ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢
    4. AIãŒç¤¾å†…æ–‡æ›¸ã‚’å‚ç…§ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
    
    ### ğŸ’¡ æ³¨æ„äº‹é …
    - OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆGPT-4ã‚’ä½¿ç”¨ï¼‰
    - åˆå›å®Ÿè¡Œæ™‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™
    - æ–‡æ›¸ã¯è‡ªå‹•çš„ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚Œã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™
    """)


if __name__ == "__main__":
    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs("./company_db", exist_ok=True)
    main()
